{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8ea562ff",
   "metadata": {},
   "source": [
    "## TAB PyTorch Extension Show Case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "db1a06b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Function\n",
    "\n",
    "import TAB_CUDA as TAB"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b950ba36",
   "metadata": {},
   "source": [
    "### List the APIs of TAB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a370689c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Conv2d', 'Quantize', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__']\n"
     ]
    }
   ],
   "source": [
    "print(dir(TAB))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fddc6aba",
   "metadata": {},
   "source": [
    "### Test the Quantization function\n",
    "\n",
    "TAB.Quantize(torch::Tensor X, torch::Tensor thresholds, int bitwidth, int N, int H, int W, int C)\n",
    "\n",
    "Return std::vector\\<torch::Tensor\\>: QW and BTN_W. \n",
    "\n",
    "BTN_W is only used in BTN, we add it in the function for unified API\n",
    "\n",
    "Ternarize: qx= +1, x>ths; qx= -1, x<-ths; qx= 0, otherwise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2db3daad",
   "metadata": {},
   "outputs": [],
   "source": [
    "KN=64\n",
    "KH=3\n",
    "KW=3\n",
    "KC=256\n",
    "bitwidth=2\n",
    "w=torch.rand([KN,KH,KW,KC])\n",
    "w_ths=0.5*torch.ones([KN])\n",
    "QW, BTN_W = TAB.Quantize(w.cuda(),w_ths.cuda(),bitwidth, KN, KH, KW, KC)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58eabf20",
   "metadata": {},
   "source": [
    "Show the Size of the quantized tensor\n",
    "\n",
    "The first bits of QW are zeros which means the quantized values only contain +1 and 0, because torch.rand() only produce values in (0,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8a39e306",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 3, 3, 4, 2]) torch.Size([64])\n",
      "torch.int64 torch.cuda.LongTensor\n",
      "tensor([[                   0, -6706628582164491276],\n",
      "        [                   0, -5642919529752776307],\n",
      "        [                   0,  6859525101148296204],\n",
      "        [                   0, -4685999609872530256]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "print(QW.size(),BTN_W.size())\n",
    "print(QW.dtype, QW.type())\n",
    "print(QW[0, 0, 0, :, :])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c194e6f",
   "metadata": {},
   "source": [
    "If we do binarization using the same data, then there will be +1 and -1 in the result\n",
    "\n",
    "Binarize: qx=+1, x > ths; qx=-1 otherwise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "95d71b66",
   "metadata": {},
   "outputs": [],
   "source": [
    "bitwidth=1\n",
    "QW, BTN_W = TAB.Quantize(w.cuda(),w_ths.cuda(),bitwidth, KN, KH, KW, KC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f977e87c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QW.size= torch.Size([64, 3, 3, 4, 1])\n",
      "torch.int64 torch.cuda.LongTensor\n",
      "tensor([[ 6706628582164491275],\n",
      "        [ 5642919529752776306],\n",
      "        [-6859525101148296205],\n",
      "        [ 4685999609872530255]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "print(\"QW.size=\", QW.size())\n",
    "print(QW.dtype, QW.type())\n",
    "print(QW[0, 0, 0, :, :])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "543c1c65",
   "metadata": {},
   "source": [
    "### Test the Conv2d function\n",
    "\n",
    "TAB.Conv2d(torch::Tensor X, torch::Tensor QW, torch::Tensor thresholds, torch::Tensor btn, \n",
    "int type, int padding1, int padding2, int stride1, int stride2, int N,  int H, int W, int C, int KN, int KH, int KW)\n",
    "\n",
    "Return the Conv2d result tensor\n",
    "\n",
    "Type: 0: TNN, 1: TBN, 2, BTN, 3: BNN\n",
    "\n",
    "TBN: Ternary-activation Binary-weight Network\n",
    "\n",
    "BTN: Binary-activation Ternary-weight Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4c9ea236",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Config the activation and weitht tnesors shapes\n",
    "N=16\n",
    "H=112\n",
    "W=112\n",
    "C=256\n",
    "\n",
    "pad1=1\n",
    "pad2=1\n",
    "str1=1\n",
    "str2=1\n",
    "\n",
    "conv_type=0\n",
    "\n",
    "x=torch.rand([N,H,W,C])\n",
    "x_ths=0.5*torch.ones([N])\n",
    "y=TAB.Conv2d(x.cuda(), QW.cuda(), x_ths.cuda(), BTN_W.cuda(), conv_type, pad1, pad2, str1, str2, N, H, W, C, KN, KH, KW)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd179a33",
   "metadata": {},
   "source": [
    "Show the Size of the conv result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "46ce500f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 64, 112, 112])\n"
     ]
    }
   ],
   "source": [
    "print(y.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1286aa7f",
   "metadata": {},
   "source": [
    "### Other cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "990645a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 64, 112, 112])\n"
     ]
    }
   ],
   "source": [
    "# TBN mode\n",
    "conv_type=1\n",
    "y=TAB.Conv2d(x.cuda(), QW.cuda(), x_ths.cuda(), BTN_W.cuda(), conv_type, pad1, pad2, str1, str2, N, H, W, C, KN, KH, KW)\n",
    "print(y.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f632f18b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 64, 112, 112])\n"
     ]
    }
   ],
   "source": [
    "# BTN mode\n",
    "conv_type=2\n",
    "y=TAB.Conv2d(x.cuda(), QW.cuda(), x_ths.cuda(), BTN_W.cuda(), conv_type, pad1, pad2, str1, str2, N, H, W, C, KN, KH, KW)\n",
    "print(y.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "66356fab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 3, 3, 4, 1])\n",
      "torch.Size([16, 64, 112, 112])\n",
      "tensor([ -6.,  20.,  32.,  22.,  38.,   8., -22.,  -4., -58.,  34., -62.,  10.,\n",
      "          0., -26., -38., -36.], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "# BNN mode\n",
    "bitwidth=1\n",
    "QW, BTN_W = TAB.Quantize(w.cuda(),w_ths.cuda(),bitwidth, KN, KH, KW, KC)\n",
    "print(QW.size())\n",
    "\n",
    "conv_type=3\n",
    "y=TAB.Conv2d(x.cuda(), QW.cuda(), x_ths.cuda(), BTN_W.cuda(), conv_type, pad1, pad2, str1, str2, N, H, W, C, KN, KH, KW)\n",
    "print(y.size())\n",
    "print(y[:,0,0,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1251d396",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae3f2c25",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e012079d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
